{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "from address_suffixes import address_suffixes\n",
    "import os\n",
    "import string\n",
    "from pandas import ExcelWriter\n",
    "from xlsxwriter.utility import xl_col_to_name\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment ID used in both new and old message name structures. \n",
    "SEGlist = [\n",
    "    'OA','O150','O180','O90','O60','O45','O30','O21','O15','O14','O10','O7','O1','O5','O120','O3','O160','O31','O365',\n",
    "    'C','C30','C15','C14','C7','C17','C21',\n",
    "    'A90','A60','A30','A21','A15','A14','A7','A10',\n",
    "    'M','M5','M5.O21','M5.O30','M4','M3.O21','M10.O30','M10.O21','M10','MI','MD','M1','M3','MI2',\n",
    "    'TEST','T','T1',\n",
    "    'CO30','CO1','CM','CO1',\n",
    "    'W','WRMUP',\n",
    "    'ACTIVE', \n",
    "    'NO30',\n",
    "    'M5.O14','M3.O14',\n",
    "]\n",
    "\n",
    "# Drop ID used in old message name structure: add new ID into this list before running error check.\n",
    "Droplist = [\n",
    "    '0','1','2','3','4','5','6','7','8','9','10','11',\n",
    "    '1A','1B','2A','2B','3A','3B','4A','4B','5A','5B','6A','6B','7A','7B','8A','8B','9A','9B','10A','10B','11A','11B',\n",
    "    'M','M1','M2','M3','M4','TEST','TEST2','T1',\n",
    "    'P','W','MI','MA','E','RD','OT','CT','I','PI','MG',\n",
    "    'MI1','MI2',\n",
    "]\n",
    "\n",
    "# Drop ID used in new message name structure: add new ID into this list before running error check.\n",
    "DroplistV2 = [\n",
    "    'P','W','MI','MA','E','RD','OT','CT','I','PI','MG','ST','T',\n",
    "]\n",
    "\n",
    "# # Creative type used in new message name structure.\n",
    "# creativelist = [\n",
    "#     'HTML',\n",
    "#     'CC',\n",
    "#     'HF.HR.GN\"Nepka 2, Nepka 1\"4372135','HF.HR.FR\"Nepka 2, Nepka 1\"437287',\n",
    "# ]\n",
    "\n",
    "SUBJlist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getFolderNames(file):\n",
    "#     '''\n",
    "    \n",
    "#     '''\n",
    "#     path = os.path.dirname(file)\n",
    "#     folder_name = os.path.basename(path)\n",
    "#     fullTime = folder_name.split('-',1)[1]\n",
    "#     endTime = folder_name.split('-')[2]\n",
    "#     prefix = folder_name.split('-')[0]\n",
    "#     return folder_name, fullTime, endTime, prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findReplaceDict = OrderedDict([\n",
    "    # populuate this dictionary with find and replace values\n",
    "    # use OrderedDict unless running Python 3.7+\n",
    "    (r\"^.*?[\\._]\\d_(.*?)_\\d+_.*$\",  r\"\\1\"),  # get just the subject\n",
    "    (r\"%%.*?%%\",  \"\"),  # remove template strings\n",
    "    (r\"^['|']s\",  \"\"),  # beginning 's\n",
    "    (r\" ['|']s\",  \"\"),  # hanging 's\n",
    "    (r\"'\",  \"'\"),  # standard apostrophe\n",
    "    (r\"-\",  \"\"),  # hyphen\n",
    "    (r\"^,\",  \"\"),  # hanging commas\n",
    "    (r\"\\.$\",  \"\"),  # ending periods\n",
    "    # remove formatted date\n",
    "    (r\"(?i)(January|February|March|April|May|June|July|August|September|October|November|December) \\d+, \\d+\",  \"\"),\n",
    "    (r\"(?i)(January|February|March|April|May|June|July|August|September|October|November|December)( \\d+)?(st|nd|rd|th)?\",  \"\"),\n",
    "    (f\"(?i)[REDACTED] {'|'.join(address_suffixes.split())}\",  \"\"),  # remove street suffix\n",
    "    (r\"\\[REDACTED\\]\",  \"\"),  # remove [REDACTED]\n",
    "    # remove street suffixes, add more between pipes\n",
    "    (r\"^\\s+\",  \"\"),  # beginning spaces\n",
    "    (r\"\\s+$\",  \"\"),  # trailing spaces\n",
    "    (r\"\\s{2,}\",  \" \"),  # remove multiple spaces, should be last run\n",
    "])\n",
    "\n",
    "\n",
    "def replaceCol(dfColumn, regex, value=r'\\1'):\n",
    "    '''\n",
    "    Regex find and replace. Use `(?i)` flag in regex for case insensitivity, otherwise use pd.Series.str.replace\n",
    "    dfColumn - the pd.DataFrame column for the regex replace\n",
    "    regex - regular expression string\n",
    "    value - \"replace with\" value; defaults to first capture group\n",
    "    '''\n",
    "    dfColumn.replace(\n",
    "        regex=True,\n",
    "        inplace=True,\n",
    "        to_replace=regex,\n",
    "        value=value\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SubjectBreaker(bronto,domainNames):\n",
    "    '''\n",
    "    break message name down to get more features, eg: date, dropID, subject line, campaign ID,...\n",
    "    '''\n",
    "    bronto['Message'] = bronto['Message'].str.lstrip(' ').str.rstrip(' ')\n",
    "    j = 1\n",
    "    for i in ['date_pt','drop','subject line','Campaign ID','rest_']:\n",
    "        bronto.insert(0, i, bronto.Message)\n",
    "#         replaceCol(bronto[i], regex=r\"(^.*?)_(\\d+[\\w]?)_(.*?)_(\\d+)_(.*)$\", value=r\"\\%s\" % j)\n",
    "#         replaceCol(bronto[i], regex=r\"(^.*?)_(.*?)_(.*?)_(\\d+)_(.*)$\", value=r\"\\%s\" % j)\n",
    "        replaceCol(bronto[i], regexcombine_bronto_ea\", value=r\"\\%s\" % j)\n",
    "        j = j+1 \n",
    "    \n",
    "    #get domain\n",
    "    bronto['domain'] = bronto.rest_.str.split('_',3).str[2]\n",
    "    bronto['domain'] = bronto.domain.astype('category').cat.rename_categories(domainNames)\n",
    "\n",
    "    return bronto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OfficeSplit(masterFile,domainHeaderName):\n",
    "    '''\n",
    "    Catogorize all the messages in ESPs to Irvine and Venice office\n",
    "    Output: \n",
    "        IrvineFile: messages dropped by Irvine office\n",
    "        VeniceFile: messages dropped by Venice office\n",
    "    '''\n",
    "    IrvineFile = masterFile[masterFile[f'{domainHeaderName}'].isin(\n",
    "        [\n",
    "          'apply-portal.net',\n",
    "            'mortgage-assisting.com',\n",
    "            'fha-guide.com',\n",
    "            'app-portal.net',\n",
    "            'thepaleo.net',\n",
    "            'house-goals.com',\n",
    "            'yourupdatereport.com',\n",
    "            'thefhacapital.com',\n",
    "        ]\n",
    "    )]\n",
    "#     df1[~df1.index.isin(df2.index)]\n",
    "    VeniceFile = masterFile[~masterFile.index.isin(IrvineFile.index)]\n",
    "    return IrvineFile,VeniceFile\n",
    "\n",
    "def RevenueSplit(masterFile):\n",
    "    '''\n",
    "    Catogorize all the messages in revenue file to Irvine and Venice office\n",
    "    Output: \n",
    "        IrvineRevFile: messages dropped by Irvine office\n",
    "        VeniceRevFile: messages dropped by Venice office\n",
    "    '''\n",
    "    IrvineRevFile = masterFile[masterFile.data_provider.isin([\n",
    "        'PMG.RF',\n",
    "        'WC.RF',\n",
    "        'UPSD.RF',\n",
    "        'PMG.DEBT',\n",
    "        'LXCN.PA',\n",
    "        'LPG.RF',\n",
    "        'LPG.FHA',\n",
    "        'AP.I',\n",
    "        'SC.RF',\n",
    "        'SC.FHA'\n",
    "    ])]\n",
    "    VeniceRevFile = masterFile[~masterFile.index.isin(IrvineRevFile.index)]\n",
    "    return IrvineRevFile, VeniceRevFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rest_Breaker(df):\n",
    "    '''\n",
    "    In SubjectBreaker function, we got one feature called rest_. This funtion further breaks rest_ down to more features\n",
    "    Output: \n",
    "        df: dataframe with more features(cols). \n",
    "    '''\n",
    "    df['esp'] = df['rest_'].str.split('_',7).str[3]\n",
    "    df['sa'] = df['rest_'].str.split('_',7).str[4]\n",
    "    df['pubid'] = df['rest_'].str.split('_',7).str[5]\n",
    "    df['creativeType'] = df['rest_'].str.split('_',7).str[6]\n",
    "    \n",
    "    co = ['esp', 'sa', 'pubid', 'creativeType']\n",
    "    df[co] = df[co].where(df['drop'].str.contains(\".C\"))\n",
    "    df.esp = np.where(df.esp.isnull(), df.ESP, df.esp)\n",
    "    df['Content ID'] = np.where(df['Content ID'].isnull(), df['creativeType'], df['Content ID'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def namingErrorV2(df,DPDSDomainESPSAPUBlist,domainNames,eaDomains):\n",
    "    '''\n",
    "    Checks error for messages with NEW naming sturcture \n",
    "    (eg:12.7.19_P.1.S.1_Reconsolidate your debt now to save the most money_5030_PMG.DEBT_O30_4_BR_AP2_460641_HTML_22NOV19)\n",
    "    Output: \n",
    "        errorDF: table of error report. Check \"Irvine & Venice Combined Files Days Error Messages by Drops (new structure)\" for example. \n",
    "    '''\n",
    "    \n",
    "    # 12.7.19_P.1.S.1_Reconsolidate your debt now to save the most money_5030_PMG.DEBT_O30_4_BR_AP2_460641_HTML_22NOV19\n",
    "    \n",
    "    df = df[['Message']]\n",
    "    df = SubjectBreaker(df,domainNames)\n",
    "    df['domain'] = df['domain'].astype(str)\n",
    "    df['domain_ID'] = df.rest_.str.split('_',3).str[2]\n",
    "    df['date_brt'] = pd.to_datetime(df.date_pt, format = '%m.%d.%y',errors='coerce')\n",
    "    \n",
    "    df['dropID'] = df['drop'].str.split('.',3).str[0]\n",
    "    df['dropNumber'] = df['drop'].str.split('.',3).str[1]\n",
    "    df['splitID'] = df['drop'].str.split('.',3).str[2]\n",
    "    df['splitNumber'] = df['drop'].str.split('.',3).str[3]\n",
    "    \n",
    "#     df['dataset'] = df.rest_.str.split('_',7).str[0]\n",
    "    df['dataset'] = df.rest_.str.split('_',7).str[0].str.rstrip(' .?!-').str.upper().str.replace('AP','AP.I').str.replace('AP.I.I','AP.I').str.replace('LXCN','LXCN.PA').str.replace('LXCN.PA.PA','LXCN.PA').str.replace('I.CARDAP.IP','I.CARDAPP')\n",
    "    df['openers'] = df.rest_.str.split('_',7).str[1]\n",
    "    df['esp'] = df.rest_.str.split('_',7).str[3]\n",
    "    df['sa'] = df.rest_.str.split('_',7).str[4]\n",
    "    df['pubid'] = df.rest_.str.split('_',7).str[5]\n",
    "    df['creativeType'] = df.rest_.str.split('_',7).str[6]\n",
    "    \n",
    "    \n",
    "#     df.insert(\n",
    "#         0, \"dataset\", \n",
    "#         df.rest_.str.split(\"_\", expand = True)[0].str.rstrip(' .?!-').str.upper().str.replace('AP','AP.I').str.replace('AP.I.I','AP.I').str.replace('LXCN','LXCN.PA').str.replace('LXCN.PA.PA','LXCN.PA').str.replace('I.CARDAP.IP','I.CARDAPP'), \n",
    "#         allow_duplicates = True\n",
    "#     )\n",
    "#     df.insert(\n",
    "#         1, \"openers\", df.rest_.str.split(\"_\", expand = True)[1].str.rstrip(' .?!-').str.upper(), allow_duplicates = True\n",
    "#     )\n",
    "    df.insert(\n",
    "        0, \"DP.DS_Domain_ESP_SA_PUBID\",df.dataset+'_'+df.domain_ID+'_'+df.esp+'_'+df.sa+'_'+df.pubid\n",
    "    )\n",
    "\n",
    "    # create error message report\n",
    "    df.is_copy = False\n",
    "    df['date_error'] = np.where(\n",
    "        df.date_brt.isnull(),\n",
    "        'Invalid date format',\n",
    "        ''\n",
    "    )\n",
    "    df['dropID_error'] = np.where(\n",
    "        df['dropID'].isin(DroplistV2),\n",
    "        '',\n",
    "        'Invalid drop ID: '+df['dropID']\n",
    "#         f\"Invalid drop ID: {df['dropID']}\"\n",
    "    )\n",
    "    df['dropNumber_error'] = np.where(\n",
    "        df['dropNumber'].isin([str(x) for x in list(range(100))]),\n",
    "        '',\n",
    "        'Invalid drop number: '+df['dropNumber'].astype(str)\n",
    "#         f\"Invalid drop number: {df['dropNumber']}\"\n",
    "    )\n",
    "    df['splitID_error'] = np.where(\n",
    "        df['splitID'].isin(['C']),\n",
    "        '',\n",
    "        'Invalid split ID: '+df['splitID'].astype(str)\n",
    "#         f\"Invalid split ID: {df['splitID']}\"\n",
    "    )\n",
    "    df['splitNumber_error'] = np.where(\n",
    "        df['splitNumber'].isin([str(x) for x in list(range(100))]),\n",
    "        '',\n",
    "        'Invalid split number: '+df['splitNumber'].astype(str),\n",
    "#         f\"Invalid split number: {df['splitNumber']}\"\n",
    "    )\n",
    "    df['campaignID_error'] = np.where(\n",
    "        df['Campaign ID'].str.len() <= 4,\n",
    "        '',\n",
    "        'Invalid campaign ID: '+df['Campaign ID']\n",
    "    ) # assume the length of campaign id is 4 digits\n",
    "    \n",
    "    df['domain_error'] = np.where(\n",
    "        df.domain.isin(eaDomains),\n",
    "        '',\n",
    "        np.where(\n",
    "            df.domain.isin(list(domainNames.values())),\n",
    "            'Domain missing in EA:'+df.domain,\n",
    "            'Invalid domain:'+df.domain\n",
    "        )\n",
    "    )\n",
    "    df['DP.DS_Domain_ESP_SA_PUBID_error'] = np.where(\n",
    "        df['DP.DS_Domain_ESP_SA_PUBID'].isin(DPDSDomainESPSAPUBlist),\n",
    "        '',\n",
    "        'Invalid DP.DS_Domain_ESP_SA_PUBID: '+df['DP.DS_Domain_ESP_SA_PUBID']\n",
    "    )\n",
    "\n",
    "    df['segment_error'] = np.where(\n",
    "        df.openers.isin(SEGlist),\n",
    "        '',\n",
    "        'Invalid segment ID: '+df.openers\n",
    "    )\n",
    "#     df['creativeType_error'] = np.where(\n",
    "#         df['creativeType'].isin(creativelist),\n",
    "#         '',\n",
    "#         'Invalid creativeType: '+df.creativeType.astype(str)\n",
    "# #         f\"Invalid creative type: {df['creativeType']}\"\n",
    "#     )\n",
    "    df['pubid_matching_error'] = ''\n",
    "\n",
    "    errorDF = df[[\n",
    "        'Message','date_error','dropID_error','dropNumber_error','splitID_error','splitNumber_error',\n",
    "        'campaignID_error','domain_error','DP.DS_Domain_ESP_SA_PUBID_error','segment_error','pubid_matching_error'\n",
    "#         'creativeType_error',\n",
    "    ]]\n",
    "    errorChecker = errorDF.drop('Message',axis = 1)\n",
    "    errorDF = errorDF[(errorChecker.values != '').any(1)]\n",
    "    return errorDF    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def namingError(df,DPDSDomainlist,domainNames,eaDomains):\n",
    "    '''\n",
    "    Checks error for messages with OLD naming sturcture \n",
    "    (eg: 12.7.19_P1_Reconsolidate your debt now to save the most money_5030_PMG.DEBT_O30_4_22NOV19)\n",
    "    Output: \n",
    "        errorDF: table of error report. Check \"Irvine & Venice Combined Files Days Error Messages by Drops (old structure)\" for example. \n",
    "    '''\n",
    "    df = df[['Message']]\n",
    "    df = SubjectBreaker(df,domainNames)\n",
    "    df['domain'] = df['domain'].astype(str)\n",
    "    df['domain_ID'] = df.rest_.str.split('_',3).str[2]\n",
    "    df['date_brt'] = pd.to_datetime(df.date_pt, format = '%m.%d.%y',errors='coerce')\n",
    "    # df[['dataset','openers']]=df.rest_.str.split('_',expand=True)[[0,1]]\n",
    "    df.insert(\n",
    "        0, \"dataset\", \n",
    "        df.rest_.str.split(\"_\",1).str[0].str.rstrip(' .?!-').str.upper().str.replace('AP','AP.I').str.replace('AP.I.I','AP.I').str.replace('LXCN','LXCN.PA').str.replace('LXCN.PA.PA','LXCN.PA').str.replace('I.CARDAP.IP','I.CARDAPP'), \n",
    "        allow_duplicates = True\n",
    "    )\n",
    "    df.insert(\n",
    "        1, \"openers\", \n",
    "        df.rest_.str.split(\"_\",2).str[1].str.rstrip(' .?!-').str.upper(), \n",
    "        allow_duplicates = True\n",
    "    )\n",
    "    df.insert(\n",
    "#         2, \"DP.DS/DV_Domain\",df.dataset+'_'+df.domain_ID.astype(int).astype(str)\n",
    "        2, \"DP.DS/DV_Domain\",df.dataset+'_'+df.domain_ID\n",
    "    )\n",
    "    \n",
    "\n",
    "#     df.insert(0, \"DP\", df.dataset.str.split(\".\", expand = True)[0].str.rstrip(' .?!-'), allow_duplicates = True)\n",
    "#     df.insert(1, \"DS\", df.dataset.str.split(\".\", expand = True)[1].str.rstrip(' .?!-'), allow_duplicates = True)\n",
    "\n",
    "    # create error message report\n",
    "    df.is_copy = False\n",
    "    df['date_error'] = np.where(\n",
    "        df.date_brt.isnull(),\n",
    "        'Invalid date format',\n",
    "        ''\n",
    "    )\n",
    "    df['dropID_error'] = np.where(\n",
    "        df['drop'].isin(Droplist),\n",
    "        '',\n",
    "        'Invalid drop ID: '+df['drop']\n",
    "    )\n",
    "    df['campaignID_error'] = np.where(\n",
    "        df['Campaign ID'].str.len() <= 4,\n",
    "        '',\n",
    "        'Invalid campaign ID: '+df['Campaign ID']\n",
    "    ) # assume the length of campaign id is 4 digits\n",
    "    \n",
    "    df['domain_error'] = np.where(\n",
    "        df.domain.isin(eaDomains),\n",
    "        '',\n",
    "        np.where(\n",
    "            df.domain.isin(list(domainNames.values())),\n",
    "            'Domain missing in EA:'+df.domain,\n",
    "            'Invalid domain:'+df.domain\n",
    "        )\n",
    "    )\n",
    "#     df['domain_error'] = np.where(\n",
    "#         df.domain.isin(list(domainNames.values())),\n",
    "#         '',\n",
    "#         'Invalid domain: '+df.domain.astype(str)\n",
    "#     )\n",
    "    df['DP.DS/DV_Domain'] = np.where(\n",
    "        df['DP.DS/DV_Domain'].isin(DPDSDomainlist),\n",
    "        '',\n",
    "        'Invalid DP.DS/DV_Domain: '+df['DP.DS/DV_Domain']\n",
    "    )\n",
    "\n",
    "    df['segment_error'] = np.where(\n",
    "        df.openers.isin(SEGlist),\n",
    "        '',\n",
    "        'Invalid segment ID: '+df.openers\n",
    "    )\n",
    "#     df['subject_error'] = np.where(\n",
    "#         df['subject line'].isin(SUBJlist),\n",
    "#         '',\n",
    "#         'Invalid subject line'\n",
    "#     )\n",
    "\n",
    "    errorDF = df[['Message','date_error','dropID_error',\n",
    "                    'campaignID_error','domain_error','DP.DS/DV_Domain','segment_error']]\n",
    "    errorChecker = errorDF.drop('Message',axis = 1)\n",
    "    errorDF = errorDF[(errorChecker.values != '').any(1)]\n",
    "    return errorDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_report(sender,revenue,fullTime,df_new, dup_full, missing_full,DPDSDomainlist,domainNames,eaDomains):\n",
    "    \"\"\"\n",
    "    For messages with OLD naming structure, get dataframes created previously to create error message report, missing message report and print a brief summary.\n",
    "    Output:\n",
    "        errorBrt: error message report for Bronto\n",
    "        errorIte: error message report for Iterable\n",
    "        errorTM: error message report for TM\n",
    "        errorrev: error message report for Revenue\n",
    "        notInRev: messages that are missing from revenue raw file\n",
    "        notInSender: messages that are missing from all ESP raw files\n",
    "        dupSender: duplicate messages in ESP raw files\n",
    "        dupRev: duplicate messages in revenue raw file\n",
    "    \"\"\"\n",
    "    # split sender data by ESP\n",
    "    Bronto = sender[sender.ESP == 'Bronto']\n",
    "    Iterable = sender[sender.ESP == 'Iterable']\n",
    "    TailoredMail = sender[sender.ESP == 'Tailored Mail']\n",
    "    \n",
    "    # Message naming error df\n",
    "    errorBrt = namingError(Bronto, DPDSDomainlist, domainNames, eaDomains)\n",
    "    errorIte = namingError(Iterable, DPDSDomainlist, domainNames, eaDomains)\n",
    "    errorTM = namingError(TailoredMail, DPDSDomainlist, domainNames, eaDomains)\n",
    "    errorrev = namingError(revenue,DPDSDomainlist,domainNames,eaDomains)\n",
    "    \n",
    "    # Missing messages df\n",
    "    checkBrt = Bronto[['Message','date_pt','rest_','drop','Campaign ID','ESP']]\n",
    "    checkIte = Iterable[['Message','date_pt','rest_','drop','Campaign ID','ESP']]\n",
    "    checkTM = TailoredMail[['Message','date_pt','rest_','drop','Campaign ID','ESP']]\n",
    "    \n",
    "    checkSender = sender[['Message','date_pt','rest_','drop','Campaign ID','ESP']]\n",
    "    checkRev = revenue[['Message','date_pt','rest_','drop','Campaign ID']]\n",
    "    notInSender = pd.merge(checkRev,checkSender,how='left', on = ['date_pt','rest_','drop','Campaign ID'], indicator = True)\n",
    "    notInSender = notInSender[notInSender._merge == 'left_only']\n",
    "\n",
    "    notInRev = pd.merge(checkSender,checkRev,how='left', on = ['date_pt','rest_','drop','Campaign ID'], indicator = True)\n",
    "    notInRev = notInRev[notInRev._merge == 'left_only']\n",
    "\n",
    "    intersect = pd.merge(checkSender,checkRev,how='inner', on = ['date_pt','rest_','drop','Campaign ID'], indicator = True)\n",
    "    \n",
    "    # Duplicate message names \n",
    "    dupSender = checkSender.loc[checkSender.duplicated([\n",
    "        'date_pt',\n",
    "        'rest_',\n",
    "        'drop',\n",
    "        'Campaign ID'\n",
    "    ], keep = False)]\n",
    "    dupRev = checkRev.loc[checkRev.duplicated([\n",
    "        'date_pt',\n",
    "        'rest_',\n",
    "        'drop',\n",
    "        'Campaign ID'\n",
    "    ], keep = False)]\n",
    "    \n",
    "    # brief summary of all types of errors.\n",
    "    print(\"\\n================ Report ===============\\n\")\n",
    "    print(\"Date range:\",fullTime)\n",
    "    print(\"\\n-------------------------------------\\n\")\n",
    "    print(\"Total Bronto drops:\",len(checkBrt))\n",
    "    print(\"Total Iterable drops:\",len(checkIte))\n",
    "    print(\"Total Tailored Mail drops:\",len(checkTM))\n",
    "    print(\"Total Revenue drops:\",len(checkRev))\n",
    "    print(\"\\n-------------------------------------\\n\")\n",
    "    print(\"ESPs non-unique dops:\",len(dupSender))\n",
    "    print(\"Revenue non-unique dops:\",len(dupRev))\n",
    "    print(\"\\n-------------------------------------\\n\")\n",
    "    print(\"When matching ESPs with Revenue report:\")\n",
    "    print(\"Drops in ESPs not in Revenue file:\",len(notInRev))\n",
    "    print(\"Drops in Revenue not in ESPs file:\",len(notInSender))\n",
    "    print(\"Drops in both ESPs and Revenue files:\", len(intersect))\n",
    "    print(\"\\n-------------------------------------\\n\")\n",
    "    print(\"Error messages in Bronto file:\",len(errorBrt))\n",
    "    print(\"Error messages in Iterable file:\",len(errorIte))\n",
    "    print(\"Error messages in TM file:\",len(errorTM))\n",
    "    print(\"Error messages in Revenue file:\",len(errorrev))\n",
    "    print(\"\\n-------------------------------------\\n\")\n",
    "    print(\"When matching ESPs with EA:\")\n",
    "    print(\"# of one-to-one matches:\",len(df_new))\n",
    "    print(\"# of one-to-many matches:\",len(dup_full.drop_duplicates('index_b',keep='first')))\n",
    "    print(\"Drops in ESPs not in EA file:\",len(missing_full))\n",
    "    print(\"\\n========== Files are attached ==========\\n\")\n",
    "    \n",
    "    return errorBrt, errorIte, errorTM, errorrev, notInRev, notInSender,dupSender, dupRev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_reportV2(sender,revenue,fullTime,df_new, dup_full, missing_full,DPDSDomainESPSAPUBlist,domainNames,eaDomains):\n",
    "    \"\"\"\n",
    "    For messages with NEW naming structure, get dataframes created previously to create error message report, missing message report and print a brief summary.\n",
    "    Output:\n",
    "        errorBrt: error message report for Bronto\n",
    "        errorIte: error message report for Iterable\n",
    "        errorTM: error message report for TM\n",
    "        errorrev: error message report for Revenue\n",
    "        notInRev: messages that are missing from revenue raw file\n",
    "        notInSender: messages that are missing from all ESP raw files\n",
    "        dupSender: duplicate messages in ESP raw files\n",
    "        dupRev: duplicate messages in revenue raw file\n",
    "    \"\"\"\n",
    "    # split sender data by ESP\n",
    "    Bronto = sender[sender.ESP == 'Bronto']\n",
    "    Iterable = sender[sender.ESP == 'Iterable']\n",
    "    TailoredMail = sender[sender.ESP == 'Tailored Mail']\n",
    "    \n",
    "    # Message naming error df\n",
    "    errorBrt = namingErrorV2(Bronto, DPDSDomainESPSAPUBlist, domainNames, eaDomains)\n",
    "    errorIte = namingErrorV2(Iterable, DPDSDomainESPSAPUBlist, domainNames, eaDomains)\n",
    "    errorTM = namingErrorV2(TailoredMail, DPDSDomainESPSAPUBlist, domainNames, eaDomains)\n",
    "    errorrev = namingErrorV2(revenue,DPDSDomainESPSAPUBlist,domainNames,eaDomains)\n",
    "    \n",
    "    ## add an extra pubid check for revenue: there is a affiliate ID column in revenue raw file, here is to check if this affiliate ID in revenue is the same as the pubid in message name.\n",
    "    revenue['pubid'] = revenue.rest_.str.split('_',7).str[5]\n",
    "    revenue['pubid'] = pd.to_numeric(revenue['pubid'], errors = 'coerce')\n",
    "    pubidCheckRev = revenue[[\n",
    "        'Message','pubid','Affiliate ID'\n",
    "    ]]\n",
    "    pubidCheckRev['pubid_matching_error'] = np.where(\n",
    "        pubidCheckRev['pubid'] == pubidCheckRev['Affiliate ID'],\n",
    "        '',\n",
    "        'pubid & affiliate id not matching'\n",
    "    )\n",
    "    \n",
    "    columns_add = [\n",
    "        'date_error','dropID_error','dropNumber_error','splitID_error','splitNumber_error',\n",
    "        'campaignID_error','domain_error','DP.DS_Domain_ESP_SA_PUBID_error','segment_error',\n",
    "    ]\n",
    "    for col in columns_add:\n",
    "        pubidCheckRev[f'{col}'] = ''\n",
    "        \n",
    "    pubidCheckRev = pubidCheckRev.reindex(columns = errorrev.columns)\n",
    "    errorCheckerPubid = pubidCheckRev.drop('Message',axis = 1)\n",
    "    pubidCheckRev = pubidCheckRev[(errorCheckerPubid.values != '').any(1)]\n",
    "    ## combine two parts of revenue error reports to make the full rev error report.\n",
    "    errorrev = pd.concat([errorrev,pubidCheckRev])\n",
    "    \n",
    "    \n",
    "    # Missing messages df\n",
    "    checkBrt = Bronto[['Message','date_pt','rest_','drop','Campaign ID','ESP']]\n",
    "    checkIte = Iterable[['Message','date_pt','rest_','drop','Campaign ID','ESP']]\n",
    "    checkTM = TailoredMail[['Message','date_pt','rest_','drop','Campaign ID','ESP']]\n",
    "    \n",
    "    checkSender = sender[['Message','date_pt','rest_','drop','Campaign ID','ESP']]\n",
    "    checkRev = revenue[['Message','date_pt','rest_','drop','Campaign ID']]\n",
    "    notInSender = pd.merge(checkRev,checkSender,how='left', on = ['date_pt','rest_','drop','Campaign ID'], indicator = True)\n",
    "    notInSender = notInSender[notInSender._merge == 'left_only']\n",
    "\n",
    "    notInRev = pd.merge(checkSender,checkRev,how='left', on = ['date_pt','rest_','drop','Campaign ID'], indicator = True)\n",
    "    notInRev = notInRev[notInRev._merge == 'left_only']\n",
    "\n",
    "    intersect = pd.merge(checkSender,checkRev,how='inner', on = ['date_pt','rest_','drop','Campaign ID'], indicator = True)\n",
    "    \n",
    "    # Duplicate message names \n",
    "    dupSender = checkSender.loc[checkSender.duplicated([\n",
    "        'date_pt',\n",
    "        'rest_',\n",
    "        'drop',\n",
    "        'Campaign ID'\n",
    "    ], keep = False)]\n",
    "    dupRev = checkRev.loc[checkRev.duplicated([\n",
    "        'date_pt',\n",
    "        'rest_',\n",
    "        'drop',\n",
    "        'Campaign ID'\n",
    "    ], keep = False)]\n",
    "    \n",
    "    \n",
    "    print(\"\\n================ Report ===============\\n\")\n",
    "    print(\"Date range:\",fullTime)\n",
    "    print(\"\\n-------------------------------------\\n\")\n",
    "    print(\"Total Bronto drops:\",len(checkBrt))\n",
    "    print(\"Total Iterable drops:\",len(checkIte))\n",
    "    print(\"Total Tailored Mail drops:\",len(checkTM))\n",
    "    print(\"Total Revenue drops:\",len(checkRev))\n",
    "    print(\"\\n-------------------------------------\\n\")\n",
    "    print(\"ESPs non-unique dops:\",len(dupSender))\n",
    "    print(\"Revenue non-unique dops:\",len(dupRev))\n",
    "    print(\"\\n-------------------------------------\\n\")\n",
    "    print(\"When matching ESPs with Revenue report:\")\n",
    "    print(\"Drops in ESPs not in Revenue file:\",len(notInRev))\n",
    "    print(\"Drops in Revenue not in ESPs file:\",len(notInSender))\n",
    "    print(\"Drops in both ESPs and Revenue files:\", len(intersect))\n",
    "    print(\"\\n-------------------------------------\\n\")\n",
    "    print(\"Error messages in Bronto file:\",len(errorBrt))\n",
    "    print(\"Error messages in Iterable file:\",len(errorIte))\n",
    "    print(\"Error messages in TM file:\",len(errorTM))\n",
    "    print(\"Error messages in Revenue file:\",len(errorrev))\n",
    "    print(\"\\n-------------------------------------\\n\")\n",
    "    print(\"When matching ESPs with EA:\")\n",
    "    print(\"# of one-to-one matches:\",len(df_new))\n",
    "    print(\"# of one-to-many matches:\",len(dup_full.drop_duplicates('index_b',keep='first')))\n",
    "    print(\"Drops in ESPs not in EA file:\",len(missing_full))\n",
    "    print(\"\\n========== Files are attached ==========\\n\")\n",
    "    \n",
    "    return errorBrt, errorIte, errorTM, errorrev, notInRev, notInSender,dupSender, dupRev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_report_saver(folder_name,prefix,version,fullTime,errorBrt, errorIte, errorTM,errorrev,notInRev, notInSender,dupSender, dupRev, missing_full, dup_fullRxmg):\n",
    "    '''\n",
    "    Write error message report, missing message report to excel. \n",
    "    Output:\n",
    "        Irvine & Venice Combined Files Days Error Messages by Drops (new structure).xlsx: new message name error report\n",
    "        Irvine & Venice Combined Files Days Error Messages by Drops (old structure).xlsx: old message name error report\n",
    "        Irvine & Venice Combined Files Missing Drops (new structure).xlsx: missing message report\n",
    "        Irvine & Venice Combined Files Missing Drops (old structure).xlsx: missing message report\n",
    "    '''\n",
    "    writer = pd.ExcelWriter(f'/Users/yanangao/Desktop/{folder_name}/{prefix} Days Error Messages by Drops {version}-{fullTime}.xlsx', engine = 'xlsxwriter')\n",
    "    workbook = writer.book\n",
    "    writer.sheets['Error Messages by Drop-Bronto'] = workbook.add_worksheet('Error Messages by Drop-Bronto')\n",
    "    writer.sheets['Error Messages by Drop-Iterable'] = workbook.add_worksheet('Error Messages by Drop-Iterable')\n",
    "    writer.sheets['Error Messages by Drop-TM'] = workbook.add_worksheet('Error Messages by Drop-TM')\n",
    "    writer.sheets['Error Messages by Drop-Revenue'] = workbook.add_worksheet('Error Messages by Drop-Revenue')\n",
    "    writer.sheets['Non-unique Messages-ESP'] = workbook.add_worksheet('Non-unique Messages-ESP')\n",
    "    writer.sheets['Non-unique Messages-Revenue'] = workbook.add_worksheet('Non-unique Messages-Revenue')\n",
    "    \n",
    "    # add subaccount col\n",
    "    errorBrt.to_excel(writer, sheet_name = 'Error Messages by Drop-Bronto',index = False)\n",
    "    \n",
    "    errorIte.to_excel(writer, sheet_name = 'Error Messages by Drop-Iterable',index = False)\n",
    "    errorTM.to_excel(writer, sheet_name = 'Error Messages by Drop-TM',index = False)\n",
    "    errorrev.to_excel(writer, sheet_name = 'Error Messages by Drop-Revenue',index = False)\n",
    "    dupSender.to_excel(writer, sheet_name = 'Non-unique Messages-ESP',index = False)\n",
    "    dupRev.to_excel(writer, sheet_name = 'Non-unique Messages-Revenue',index = False)\n",
    "    writer.save()\n",
    "\n",
    "\n",
    "    writer = pd.ExcelWriter(f'/Users/yanangao/Desktop/{folder_name}/{prefix} Missing Drops {version}-{fullTime}.xlsx', engine = 'xlsxwriter')\n",
    "    workbook = writer.book\n",
    "    writer.sheets['Drops Missing in Revenue'] = workbook.add_worksheet('Drops Missing in Revenue')\n",
    "    writer.sheets['Drops Missing in ESPs'] = workbook.add_worksheet('Drops Missing in ESPs')\n",
    "\n",
    "    notInRev[['Message_x','ESP']].to_excel(writer, sheet_name = 'Drops Missing in Revenue',index = False)\n",
    "    notInSender[['Message_x']].to_excel(writer, sheet_name = 'Drops Missing in ESPs',index = False)\n",
    "#     missing_full[['Message']].to_excel(writer, sheet_name = 'Drops Missing in EA',index = False)\n",
    "#     # add duplicate drops in EA\n",
    "#     dup_fullRxmg[['Message']].to_excel(writer, sheet_name = 'Drops Duplicated in EA', index = False)\n",
    "    \n",
    "    \n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_missing_ea(missing_fullRxmg,dup_fullRxmg, brontoRxmg, DPDSDomainESPSAPUBlist, domainNames, eaDomains):\n",
    "    '''\n",
    "    Create a summary for messages don't have inboxing stats, at DP.DS/DV and segment level.\n",
    "    Output: There are 4 stats for each table:\n",
    "                All messages: all the messages that are missing inbox stats\n",
    "                # of duplicates: missing because of duplicate matching \n",
    "                # of missing domains: missing because of missing EA domains\n",
    "                # of missing messages: missing because there is no record in EA\n",
    "            df2: the above 4 stats for each [DP.DS/DV, segment] combination\n",
    "            df1: df2 further grouped by DP.DS/DV. \n",
    "\n",
    "        \n",
    "    '''\n",
    "    missing_fullRxmg['DP.DS'] = missing_fullRxmg['rest_'].str.split(\"_\").str[0]\n",
    "    missing_fullRxmg['Segment'] = missing_fullRxmg['rest_'].str.split(\"_\").str[1]\n",
    "    dup_fullRxmg['DP.DS'] = dup_fullRxmg['rest_'].str.split(\"_\").str[0]\n",
    "    dup_fullRxmg['Segment'] = dup_fullRxmg['rest_'].str.split(\"_\").str[1]\n",
    "\n",
    "    df = missing_fullRxmg[['Message','DP.DS','Segment']]\n",
    "    dup = dup_fullRxmg[['Message','DP.DS','Segment']]\n",
    "    dup['# of duplicates'] = 1\n",
    "    dup['# of missing domains'] = 0\n",
    "\n",
    "    eaDomainMiss = namingErrorV2(brontoRxmg, DPDSDomainESPSAPUBlist, domainNames, eaDomains)\n",
    "    eaDomainMiss = eaDomainMiss[eaDomainMiss.domain_error != ''][['Message']]\n",
    "\n",
    "\n",
    "    # missing drops not caused by domain missing\n",
    "    dfmerge = pd.merge(\n",
    "        df,\n",
    "        eaDomainMiss,\n",
    "        how = 'left',\n",
    "        on = 'Message',\n",
    "        indicator = True\n",
    "    )\n",
    "    #     dfleft = dfmerge[dfmerge._merge == 'left_only']\n",
    "    dfmerge['# of duplicats'] = 0\n",
    "    dfmerge['# of missing domains'] = dfmerge._merge.str.replace('left_only','0').str.replace('both','1')\n",
    "    dfmerge['# of missing domains'] = dfmerge['# of missing domains'].astype(int)\n",
    "    dfmerge.drop('_merge',axis = 1,inplace = True)\n",
    "\n",
    "\n",
    "    dfMissingFull = pd.concat([dup,dfmerge])\n",
    "    dfMissingFull = dfMissingFull.rename(columns = {'Message':'All messages'})\n",
    "\n",
    "    df2 = dfMissingFull.groupby(['DP.DS','Segment']).agg({\n",
    "        'All messages':'count',\n",
    "        '# of duplicates':'sum',\n",
    "        '# of missing domains':'sum'\n",
    "    }).reset_index()\n",
    "    df2['# of missing messages'] = df2['All messages'] - df2['# of duplicates'] - df2['# of missing domains']\n",
    "\n",
    "    df1 = dfMissingFull.groupby(['DP.DS']).agg({\n",
    "        'All messages':'count',\n",
    "        '# of duplicates':'sum',\n",
    "        '# of missing domains':'sum'\n",
    "    }).reset_index()\n",
    "    df1['Segment'] = 'All'\n",
    "    df1['# of missing messages'] = df1['All messages'] - df1['# of duplicates'] - df1['# of missing domains']\n",
    "    df1 = df1.reindex(columns=df2.columns).sort_values('All messages',ascending = False)\n",
    "    return df1, df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ea_error_report(\n",
    "    folder_name,prefix,\n",
    "    missing_fullRxmg, dup_fullRxmg, brontoRxmg, \n",
    "    DPDSDomainESPSAPUBlist, domainNames, eaDomains):\n",
    "    '''\n",
    "    Write df1 and df2 into excel, format the excel file\n",
    "    Output: EA missing drops error report.xlsx\n",
    "    '''\n",
    "    \n",
    "    df1, df2 = summary_missing_ea(missing_fullRxmg,dup_fullRxmg, brontoRxmg, DPDSDomainESPSAPUBlist, domainNames, eaDomains)\n",
    "    \n",
    "    writer = pd.ExcelWriter(f'/Users/yanangao/Desktop/{folder_name}/EA missing drops error report.xlsx', engine = 'xlsxwriter')\n",
    "    workbook = writer.book\n",
    "    \n",
    "    \n",
    "    # summary table\n",
    "    yanan = workbook.add_worksheet('Missing EA summary')\n",
    "    yanan.outline_settings(True,False,False,False)\n",
    "\n",
    "    row=0\n",
    "    col=0\n",
    "\n",
    "    titles=df2.columns\n",
    "\n",
    "    for i in titles:\n",
    "        yanan.write(row,col,i)\n",
    "        col+=1\n",
    "    row+=1\n",
    "    col=0\n",
    "    for i,l in df1.iterrows():\n",
    "        yanan.write(row,col,l[0])\n",
    "        col+=1\n",
    "        yanan.write(row,col,l[1])\n",
    "        col+=1\n",
    "        yanan.write(row,col,l[2])\n",
    "        col+=1\n",
    "        yanan.write(row,col,l[3])\n",
    "        col+=1\n",
    "        yanan.write(row,col,l[4])\n",
    "        col+=1\n",
    "        yanan.write(row,col,l[5])\n",
    "        col=0\n",
    "        row+=1\n",
    "        for k,j in df2[df2['DP.DS']==l[0]].iterrows():\n",
    "            yanan.write(row,col,j[0])\n",
    "            col+=1\n",
    "            yanan.write(row,col,j[1])\n",
    "            col+=1\n",
    "            yanan.write(row,col,j[2])\n",
    "            col+=1\n",
    "            yanan.write(row,col,j[3])\n",
    "            col+=1\n",
    "            yanan.write(row,col,j[4])\n",
    "            col+=1\n",
    "            yanan.write(row,col,j[5])\n",
    "            yanan.set_row(row,None,None,{'level':1,'hidden':True,'collapsed':True})\n",
    "            col=0\n",
    "            row+=1\n",
    "    \n",
    "    # missing EA drop list\n",
    "    missing_fullRxmg[['Message']].to_excel(writer, sheet_name = 'Drops Missing in EA',index = False)\n",
    "    # duplicates EA drop list\n",
    "    dup_fullRxmg[['Message']].to_excel(writer, sheet_name = 'Drops Duplicated in EA', index = False)\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_bronto_ea(bronto, ea):\n",
    "    '''\n",
    "    Inner join Bronto and Email Analyst DataFrames\n",
    "    bronto - pandas.DataFrame processed by data.bronto\n",
    "    ea - pandas.DataFrame processed by data.ea\n",
    "    \n",
    "    Output:\n",
    "        df_new: ESP drops that are one to one matched with EA reports\n",
    "        dup_full: ESP drops that have duplicate matches \n",
    "        missing_full: ESP drops that can't be matched with EA reports\n",
    "    '''\n",
    "    # add date plus one column\n",
    "    bronto['date_plus_one'] = bronto.date_brt + datetime.timedelta(days = 1)\n",
    "    \n",
    "    # delivered on the same day\n",
    "    combined_1 = bronto.merge(ea, how = 'inner', \n",
    "                                            left_on = ['date_brt','sub_clean', 'domain'],\n",
    "                                            right_on = ['date','sub_clean', 'sender_domain'])\n",
    "    \n",
    "    # delivered on next day\n",
    "    combined_2 = bronto.merge(ea, how = 'inner', \n",
    "                                            left_on = ['date_plus_one','sub_clean','domain'],\n",
    "                                            right_on = ['date','sub_clean','sender_domain'])\n",
    "    combined_full = pd.concat([combined_2, combined_1],ignore_index=True)\n",
    "\n",
    "    \n",
    "    #  get unmatched drops\n",
    "    non_na = combined_2['index_b'].tolist() + combined_1['index_b'].tolist()\n",
    "    complete_full = bronto.index.isin(non_na)\n",
    "    missing_full = bronto[~complete_full]\n",
    "\n",
    "      \n",
    "    # get duplicated drops\n",
    "    dup_brt = combined_full.loc[combined_full.duplicated('index_b', keep = False)]\n",
    "    dup_ibr = combined_full.loc[combined_full.duplicated('index_i', keep = False)]\n",
    "\n",
    "    dup_full = pd.concat([dup_brt, dup_ibr], ignore_index = True).drop_duplicates(keep = 'first')\n",
    "\n",
    "    # get perfectly matched data without duplicated in any file\n",
    "    df_new = pd.concat([combined_full,dup_full]).drop_duplicates(keep=False)\n",
    "    df_new.drop('openers',axis=1,inplace=True)\n",
    "    \n",
    "\n",
    "    print(\"\\n-------EA match Bronto report---------\\n\")\n",
    "    print(\"# of duplicated drops in brt:\", dup_full.drop_duplicates('index_b',keep='first').shape)\n",
    "    \n",
    "    print(\"# of good drops in brt:\", df_new.shape)\n",
    "    \n",
    "    print(\"# of unmatches in brt:\", missing_full.shape)\n",
    "   \n",
    "\n",
    "    if len(dup_full.drop_duplicates('index_b',keep='first')) + len(df_new) + len(missing_full) == len(bronto):\n",
    "        print(\"Pass row number check of brt and merging tables\")\n",
    "    else: \n",
    "        print(\"Row number error of merging table\")\n",
    "    print(\"\\n---------------------------------------\\n\")\n",
    "        \n",
    "    return df_new, dup_full, missing_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addRevenue(brontoRxmg, revenueRxmg, df_newRxmg):\n",
    "    '''\n",
    "    Notice:\n",
    "        1. We use revenue file as the source of truth. Meaning that any messages that are in ESP files but not in revenue files will be eliminated from the output.\n",
    "        2. ['sent','ccontact lost','contact loss rate','CTR'] are only in ESP dataframe; revenue stats are only in revenue file; inbox stats are only in EA file\n",
    "        3. combine the 3 dataframes to get the master stats. \n",
    "        4. ['delivered','opens','clicks'] are in both revenue and esp files, but we want to use those in esp file, because those are more up to date.\n",
    "    Output: \n",
    "        merged: a dataframe having all messages from revenue file and all stats.    \n",
    "    '''\n",
    "    # add contact loss to craig's reporting\n",
    "\n",
    "    brontoRxmg = brontoRxmg.drop('domain',axis=1)\n",
    "    # revenueRxmg = revenueRxmg.drop('subject line',axis=1)\n",
    "\n",
    "    contact = brontoRxmg[[\n",
    "        'date_pt', 'rest_', 'drop','Campaign ID',\n",
    "        'Contacts Lost','Contact Loss Rate','Click Through Rate','Sent','ESP','subject line',\n",
    "        'Delivered','Opens','Open Rate','Clicks','Click Rate'\n",
    "    ]]\n",
    "    \n",
    "    \n",
    "    # add inbox stats\n",
    "    merged1 = pd.merge(\n",
    "        contact, \n",
    "        df_newRxmg.drop([\n",
    "            'ESP','domain','subject line','Delivered','Opens','Open Rate','Clicks','Click Rate'\n",
    "        ],axis=1), \n",
    "        how = 'left', \n",
    "        on = ['date_pt', 'rest_', 'drop','Campaign ID'], \n",
    "#         indicator = True\n",
    "    )\n",
    "    \n",
    "    # add revenue\n",
    "    merged = pd.merge(\n",
    "        revenueRxmg,\n",
    "        merged1,\n",
    "        how= 'left',\n",
    "        on=['date_pt', 'rest_', 'drop','Campaign ID'],\n",
    "        indicator = True\n",
    "    ).drop(['index_craig'],axis = 1)\n",
    "    \n",
    "    #_x is from revenue, _y is from esp\n",
    "    merged['Delivered_y'].fillna(merged['Delivered_x'],inplace = True)\n",
    "    merged['Opens_y'].fillna(merged['Opens_x'],inplace = True)\n",
    "    merged['Open Rate_y'].fillna(merged['Open Rate_x'],inplace = True)\n",
    "    merged['Clicks_y'].fillna(merged['Clicks_x'],inplace = True)\n",
    "    merged['Click Rate_y'].fillna(merged['Click Rate_x'],inplace = True)\n",
    "    merged['subject line_y'].fillna(merged['subject line_x'],inplace = True)\n",
    "    \n",
    "    # _y is from esp; _x is from revenue\n",
    "    merged = merged.rename(columns={\n",
    "        'Contacts Lost_x':'Contacts Lost',\n",
    "        'Contact Loss Rate_x':'Contact Loss Rate',\n",
    "        'Click Through Rate_x':'Click Through Rate',\n",
    "        'Sent_x':'Sent',\n",
    "        'Delivered_y':'Delivered',\n",
    "        'Opens_y':'Opens',\n",
    "        'Open Rate_y':'Open Rate',\n",
    "        'Clicks_y':'Clicks',\n",
    "        'Click Rate_y':'Click Rate',\n",
    "        'subject line_y':'subject line_esp',\n",
    "        'subject line_x':'subject line_rev',\n",
    "    })\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidated_report(combine):\n",
    "    '''\n",
    "    Input:\n",
    "        combine: dataframe get from last function (addRevenue)\n",
    "    Output:\n",
    "        master: the same thing as the input table combine\n",
    "        weekly_drops: select some features out of combine df, will be write into excel: raw_no_format_02.02.20 - 03.03.20.xlsx\n",
    "        combine: select some features and format them as requsted, will be write into excel: Venice and Irvine Reporting - Consolidated Bronto SA Drop Stats_02.25.20-03.03.20.xlsx\n",
    "    '''\n",
    "    master = combine.copy()\n",
    "    # add adjusted clicks and adjusted click rate\n",
    "    combine['Adjusted Clicks'] = combine['Clicks'] - combine['Contacts Lost']\n",
    "    combine['Adjusted Clicks Rate'] = combine['Adjusted Clicks']/combine['Opens']\n",
    "    combine['ESP'] = combine['ESP'].replace(np.nan, \"Missing\")\n",
    "    \n",
    "    # ================== file without format (yanan use only)=====================\n",
    "    attr = ['Message_x','Date','drop', 'Sent','Delivered', 'Opens', 'Open Rate', 'Clicks', 'Click Rate',\n",
    "            'Contacts Lost','Contact Loss Rate','Adjusted Clicks','Adjusted Clicks Rate','Offer Name', \n",
    "            'Campaign ID', 'Link Used in Mailing', 'Affiliate ID', 'Sub ID','Content ID', #'Unnamed: 11', \n",
    "            'Revenue', 'RPC', 'Revenue CPM (eCPM)', 'Conversions', 'Cost CPM', 'Cost per send', 'Net Revenue', \n",
    "            'Margin', 'subject line_rev','subject line_esp', 'data_provider','openers' ,'aol_inbox_percent', 'actual_aol_volume', \n",
    "            'google_inbox_percent', 'actual_google_volume', 'yahoo_inbox_percent', 'actual_yahoo_volume',\n",
    "            'outlook_inbox_percent', 'actual_outlook_volume', 'global_isps_inbox_percent',\n",
    "            'actual_global_isps_volume', 'actual_overall_volume', 'overall_inbox_percent', \n",
    "            'aol_action_volume', 'google_action_volume', 'yahoo_action_volume', \n",
    "            'outlook_action_volume', 'global_isps_action_volume', 'overall_action_volume','ESP','domain','rest_']\n",
    "    weekly_drops = combine[attr].rename(columns={'Message_x':'Message'})\n",
    "    \n",
    "    \n",
    "    # ================= formatted file (google sheet 2020) ================================\n",
    "#     combine[\"DP.DS/DV_PubID\"] = combine['data_provider']+'_'+combine['data_provider']\n",
    "    attr = ['Campaign ID','Date',  'data_provider', 'openers', 'Message_x', 'Sent', 'Delivered', 'Opens', 'Open Rate', 'Clicks',\n",
    "        'Click Rate', 'Contacts Lost','Adjusted Clicks','Adjusted Clicks Rate','Offer Name', 'Campaign ID', 'Link Used in Mailing', 'Affiliate ID', 'Sub ID','Content ID', #'Unnamed: 11',\n",
    "        'Revenue', 'RPC', 'Revenue CPM (eCPM)', 'Conversions', 'Cost CPM', 'Cost per send', 'Net Revenue', \n",
    "         'overall_inbox_percent', 'aol_inbox_percent','google_inbox_percent', 'yahoo_inbox_percent', 'outlook_inbox_percent',\n",
    "        'global_isps_inbox_percent','Click Through Rate','drop','ESP','rest_']\n",
    "    \n",
    "    combine = combine[attr]\n",
    "    combine = combine.rename(columns={\n",
    "        'Message_x':'Message','Campaign ID':'Offer ID',\n",
    "        #'Unnamed: 11':np.nan,\n",
    "    })\n",
    "    combine.insert(2, \"Helper\", np.nan)\n",
    "    combine.insert(28,\"Vertical\", np.nan)\n",
    "    combine.insert(29,\"Network\", np.nan)\n",
    "    \n",
    "    \n",
    "    return combine, weekly_drops, master\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addVertical(monthly_drops,mappingDF):\n",
    "    '''\n",
    "    Add source vertical and vertical ID (originally exist in EMIT) to master stats (called weekly_drops in script)\n",
    "    Output:\n",
    "        monthlydropsCredit: master stats with two more columns (vertical and vert ID) added\n",
    "    '''\n",
    "    creditDPDS = mappingDF[[\n",
    "        'Vertical','Vertical ID','DP.DS or DP.DV if multiple sources using samePubID',\n",
    "    ]].drop_duplicates(subset = 'DP.DS or DP.DV if multiple sources using samePubID', keep = 'first')\n",
    "\n",
    "    monthlydropsCredit = monthly_drops.copy()\n",
    "    monthlydropsCredit['data_provider'] = monthlydropsCredit['data_provider'].str.upper()\n",
    "    monthlydropsCredit = pd.merge(\n",
    "        monthlydropsCredit,\n",
    "        creditDPDS,\n",
    "        how = 'left',\n",
    "        left_on = 'data_provider',\n",
    "        right_on = 'DP.DS or DP.DV if multiple sources using samePubID',\n",
    "        indicator = True\n",
    "    )\n",
    "    monthlydropsCredit.rename(columns = {'Vertical':'Source_Vertical'},inplace = True)\n",
    "    return monthlydropsCredit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_col(smry1):\n",
    "    smry1['Open rate'] = smry1['Opens']/smry1['Delivered']\n",
    "    smry1['Click rate'] = smry1['Clicks']/smry1['Opens']\n",
    "    smry1['Adjusted Click Rate'] = smry1['Adjusted Clicks']/smry1['Opens']\n",
    "    smry1['EPC'] = smry1['Revenue']/smry1['Clicks']\n",
    "    smry1['eCPM'] = smry1['Revenue']*1000/smry1['Delivered']\n",
    "    smry1['% matched drops'] = smry1['number of matched']/smry1['number of total drops']\n",
    "    for j in ['aol', 'google', 'yahoo', 'outlook', 'global_isps', 'overall']:\n",
    "        smry1['%s_inbox_percent' % j] = smry1['%s_action_volume' % j] / smry1['actual_%s_volume' % j]\n",
    "    return smry1\n",
    "\n",
    "\n",
    "def row_style(row):\n",
    "    if row['Offer Name'] == 'Total':\n",
    "        return pd.Series('background-color: yellow', row.index)\n",
    "    else:\n",
    "        return pd.Series('', row.index)\n",
    "    \n",
    "    \n",
    "\n",
    "def irvine_report_main(sub_drops, offers):\n",
    "    '''\n",
    "    Create offer performance reports\n",
    "    Output:\n",
    "        rk_offer: message stats grouped by campaign ID -> campaign ID level offer performance report\n",
    "        s_ver_1: message stats grouped by campaign ID and Vertical -> Vertical level performance report\n",
    "        third_final: a report combined campaign ID and vertical level report. vertical level report is used as a break down for campaignID level report\n",
    "    Example: check \"overall offer performance\", \"overall vertical performance\" and \"break down\" in \"Irvine - Offer & Vertical Performance by Vertical-02.02.20 - 03.03.20.xlsx\"\n",
    "    '''\n",
    "\n",
    "    # aggregate stats for messages on campaign ID\n",
    "    smry1 = sub_drops.groupby(['Campaign ID']).agg({'overall_inbox_percent':'count','Message':'count',\n",
    "                                                    'Sent':'sum',\n",
    "                                                   'Delivered':'sum','Opens':'sum','Clicks':'sum','Adjusted Clicks':'sum','Revenue':'sum',\n",
    "                                                   'actual_aol_volume':'sum','actual_google_volume':'sum',\n",
    "                                                   'actual_yahoo_volume':'sum','actual_outlook_volume':'sum',\n",
    "                                                   'actual_global_isps_volume':'sum','actual_overall_volume':'sum',\n",
    "                                                   'aol_action_volume':'sum','google_action_volume':'sum',\n",
    "                                                   'yahoo_action_volume':'sum','outlook_action_volume':'sum',\n",
    "                                                   'global_isps_action_volume':'sum','overall_action_volume':'sum',\n",
    "                                                   'Date':'max',\n",
    "                                                   }).reset_index(drop = False)\n",
    "\n",
    "    smry1 = smry1.rename(columns={'overall_inbox_percent':'number of matched','Message':'number of total drops','Date':'Last seen'})\n",
    "    smry1 = add_col(smry1)\n",
    "    attr = ['Hitpath Offer ID', 'Offer Name', 'Vertical', 'Operational Status','Advertiser Name', 'Payout', 'Payout Type', 'Custom Creative Allowed','Budget','Cap', 'Day Restrictions','Additional Notes']\n",
    "    smry2 = offers[attr]\n",
    "    smry2['live']=np.where(\n",
    "        smry2['Operational Status'] == 'Live',\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    rk_offer = pd.merge(smry1, smry2, how = 'left', left_on = 'Campaign ID', right_on = 'Hitpath Offer ID')\n",
    "    \n",
    "    # further aggregate stats on Vertical \n",
    "    s_ver_1 = rk_offer.groupby(['Vertical']).agg({'number of matched':'sum', 'number of total drops':'sum','Sent':'sum',\n",
    "                                                  'Delivered':'sum','Opens':'sum','Clicks':'sum','Adjusted Clicks':'sum','Revenue':'sum','Offer Name':'count','live':'sum',\n",
    "                                                   'actual_aol_volume':'sum','actual_google_volume':'sum',\n",
    "                                                   'actual_yahoo_volume':'sum','actual_outlook_volume':'sum',\n",
    "                                                   'actual_global_isps_volume':'sum','actual_overall_volume':'sum',\n",
    "                                                   'aol_action_volume':'sum','google_action_volume':'sum',\n",
    "                                                   'yahoo_action_volume':'sum','outlook_action_volume':'sum',\n",
    "                                                   'global_isps_action_volume':'sum','overall_action_volume':'sum',\n",
    "                                                   'Last seen':'max',\n",
    "                                                 }).reset_index(drop = False)\n",
    "    s_ver_1 = add_col(s_ver_1)\n",
    "    s_ver_1 = s_ver_1.rename(columns={'Offer Name':'Number of Offers', 'live':'Number of Live Offers'})\n",
    "    att_rk = ['Hitpath Offer ID', 'Offer Name', 'Vertical', 'Operational Status', 'Advertiser Name', 'Payout', 'Payout Type','Custom Creative Allowed','Budget','Cap', 'Day Restrictions','Additional Notes',\n",
    "            'Campaign ID', 'Sent','Delivered', 'Opens','Open rate', 'Clicks', 'Click rate','Adjusted Clicks','Adjusted Click Rate','Revenue',  'EPC', 'eCPM',\n",
    "            'aol_inbox_percent', 'google_inbox_percent', 'yahoo_inbox_percent', 'outlook_inbox_percent', 'global_isps_inbox_percent', 'overall_inbox_percent',\n",
    "             'number of matched', 'number of total drops','% matched drops','Last seen',]\n",
    "    rk_offer = rk_offer[att_rk]\n",
    "    att_sver = ['Vertical', 'Number of Offers', 'Number of Live Offers','Sent','Delivered', 'Opens','Open rate', 'Clicks', 'Click rate','Adjusted Clicks','Adjusted Click Rate','Revenue',  'EPC', 'eCPM',\n",
    "               'aol_inbox_percent', 'google_inbox_percent', 'yahoo_inbox_percent', 'outlook_inbox_percent', 'global_isps_inbox_percent', 'overall_inbox_percent',\n",
    "               'number of matched', 'number of total drops','% matched drops','Last seen',]\n",
    "    s_ver_1 = s_ver_1[att_sver]\n",
    "    \n",
    "    # on the basis of previous two dataframes (rk_offer, s_ver_1), we combined these two tables together. Each row in s_ver_1 is a *vertical summary* of the rows in rk_offer that have the same vertical.\n",
    "    header_list = list(rk_offer)\n",
    "    add_cols = s_ver_1.reindex(columns = header_list)\n",
    "    add_cols['Offer Name'] = 'Total'\n",
    "    third = pd.concat([rk_offer, add_cols]).sort_values(by = ['Vertical', 'Hitpath Offer ID'])\n",
    "    third_final = third.reset_index().drop('index', axis = 1).style.apply(row_style, axis=1) \n",
    "    return rk_offer, s_ver_1, third_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offerNotSeen(offers,rk_offer_full,rk_offer_oa,columns):\n",
    "    '''\n",
    "    Create the \"offers not seen\" tab in \"Irvine - Offer & Vertical Performance by Vertical-02.02.20 - 03.03.20.xlsx\" report\n",
    "    Output:\n",
    "        offersNotSeen: the dataframe shows the offers that have not been sent to this Office or vertical or segment yet.\n",
    "    '''\n",
    "    IrvVenMergeOffer = pd.merge(\n",
    "        offers[[\n",
    "            'Hitpath Offer ID','Offer Name','Vertical','Operational Status','Advertiser Name','Payout',\n",
    "            'Payout Type','Cap','Day Restrictions','Additional Notes'\n",
    "        ]],\n",
    "        rk_offer_full,\n",
    "        how = 'outer',\n",
    "        on = [\n",
    "            'Hitpath Offer ID','Offer Name','Vertical','Operational Status','Advertiser Name','Payout',\n",
    "            'Payout Type','Cap','Day Restrictions','Additional Notes'\n",
    "        ],\n",
    "        indicator = True\n",
    "    )\n",
    "    IrvVenMergeOffer = IrvVenMergeOffer.reindex(columns = columns)\n",
    "    \n",
    "    offersNotSeen = pd.merge(\n",
    "        IrvVenMergeOffer,\n",
    "        rk_offer_oa[['Campaign ID']],\n",
    "        how = 'left',\n",
    "        on = 'Campaign ID', \n",
    "        indicator = True\n",
    "    )\n",
    "    offersNotSeen = offersNotSeen[offersNotSeen._merge == 'left_only'].drop('_merge', axis = 1).sort_values('eCPM', ascending = False)\n",
    "    return offersNotSeen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rxmg_report(writer,IrvVenWeeklyDrops,selected_drop,criteria,segments,offers):\n",
    "    \"\"\"\n",
    "    Note:\n",
    "        1. IrvVenWeeklyDrops: this df is only used to get \"offer not seen\" tab, stats in this tab is the combined stats of both offices.\n",
    "        2. selected_drop: this df is used to produce all other tabs. Stats used in other tabs are Venice or Irvine office only.\n",
    "    Function:\n",
    "        This function saves the dataframes created by irvine_report_main to excel. \n",
    "    Output:\n",
    "        Excel report: Irvine - Offer & Vertical Performance by Vertical-02.25.20-03.03.20.xlsx\n",
    "    \"\"\"\n",
    "    # df needed forehand \n",
    "    rk_offer_full, s_ver_1_full, third_final_full = irvine_report_main(IrvVenWeeklyDrops, offers)\n",
    "    columns = list(rk_offer_full.columns)\n",
    "    \n",
    "    # add offer and vertical performance sheet\n",
    "    rk_offer_oa, s_ver_1_oa, third_final_oa = irvine_report_main(selected_drop, offers)\n",
    "    rk_offer_oa.to_excel(writer, 'overall offer performance',index = False)\n",
    "    s_ver_1_oa.to_excel(writer, 'overall vertical performance',index = False)\n",
    "    third_final_oa.to_excel(writer, 'break down',index = False)\n",
    "    \n",
    "    # add \"offers not seen\" sheet\n",
    "    not_seen = offerNotSeen(offers,rk_offer_full,rk_offer_oa,columns)\n",
    "    not_seen.to_excel(writer, 'offers not seen',index = False)\n",
    "    \n",
    "    # format % and $ sign\n",
    "    format(rk_offer_oa, writer, 'overall offer performance')\n",
    "    format(s_ver_1_oa, writer, 'overall vertical performance')\n",
    "    format(not_seen, writer, 'offers not seen')\n",
    "    \n",
    "    # break the report down by certain criteria (segment or vertical), and save to excel files.\n",
    "    for seg in segments:\n",
    "        sub_drops = selected_drop.loc[selected_drop[f'{criteria}'] == seg]\n",
    "        rk_offer, s_ver_1, third_final = irvine_report_main(sub_drops, offers)\n",
    "        rk_offer.to_excel(writer, f'{seg} offer performance',index = False)\n",
    "        s_ver_1.to_excel(writer,  f'{seg} vert performance',index = False)\n",
    "        third_final.to_excel(writer, f'{seg} break down',index = False)\n",
    "\n",
    "        # offers haven't been sent\n",
    "        not_seen = offerNotSeen(offers,rk_offer_full,rk_offer,columns)\n",
    "        not_seen.to_excel(writer, f'{seg} offers not seen',index = False)\n",
    "        \n",
    "        # format % and $ sign\n",
    "        format(rk_offer, writer, f'{seg} offer performance')\n",
    "        format(s_ver_1, writer, f'{seg} vert performance')\n",
    "        format(not_seen, writer, f'{seg} offers not seen')\n",
    "                \n",
    "    # add \"consolidated stats\" for this pubID\n",
    "    selected_drop.to_excel(writer, 'consolidated stats', index = False)\n",
    "\n",
    "    return writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_column(df: 'dataframe', worksheet: 'a pd.Excelwriter sheet', cols: list, format: 'excel format to use', col_width: int = None) -> None:\n",
    "    \"\"\" sets column by index, the column's position in the dataframe \"\"\"\n",
    "    idx = [df.columns.get_loc(c) for c in cols if c in df]\n",
    "\n",
    "    for i in idx:\n",
    "        # set the column width and format\n",
    "        col = xl_col_to_name(i)\n",
    "        worksheet.set_column(\n",
    "            f'{col}:{col}',\n",
    "            col_width,\n",
    "            format)\n",
    "\n",
    "def format(df: 'affiliate revenue stats dataframe', writer: str, sheetname: str, ) -> 'pd.ExcelWriter':\n",
    "    '''\n",
    "    formats affiliate revenue stats excel worksheet with currency and percentage formatting\n",
    "    '''\n",
    "\n",
    "    # get the xlsxwriter workbook and worksheet objects\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[sheetname]\n",
    "\n",
    "    # add format cells written\n",
    "    pct_fmt = workbook.add_format({\n",
    "        'num_format': '0.0%',\n",
    "    })\n",
    "\n",
    "    money_fmt = workbook.add_format({\n",
    "        'num_format': '$#,##0.00',\n",
    "    })\n",
    "    \n",
    "    int_fmt = workbook.add_format({\n",
    "        'num_format': '#,##0',\n",
    "    })\n",
    "\n",
    "    pct_cols = df.filter(regex='Rate|Margin|rate|percent|CLR')\n",
    "    money_cols = df.filter(regex='Revenue|RPC|Cost|CPM|EPC')\n",
    "    int_cols = df.filter(regex='Conversions')\n",
    "    \n",
    "\n",
    "    set_column(df, worksheet, pct_cols, pct_fmt)\n",
    "    set_column(df, worksheet, money_cols, money_fmt)\n",
    "    set_column(df, worksheet, int_cols, int_fmt)\n",
    "\n",
    "    return writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectLinePerformance(weekly_drops,offers):\n",
    "    \"\"\"\n",
    "    Create subject line performance report\n",
    "    Output:\n",
    "       groupMaster: check \"subject line performance_COMBINED_02.25.20-03.03.20.xlsx\" \n",
    "    \"\"\"\n",
    "    groupMaster = weekly_drops.groupby(['Campaign ID','subject line_rev']).agg({\n",
    "        'Message':'count',\n",
    "        'openers':lambda x:list(np.unique(x)),\n",
    "        'Sent':'sum','Delivered':'sum','Opens':'sum','Clicks':'sum','Adjusted Clicks':'sum','Revenue':'sum',\n",
    "        'actual_aol_volume':'sum','actual_google_volume':'sum',\n",
    "        'actual_yahoo_volume':'sum','actual_outlook_volume':'sum',\n",
    "        'actual_global_isps_volume':'sum','actual_overall_volume':'sum',\n",
    "        'aol_action_volume':'sum','google_action_volume':'sum',\n",
    "        'yahoo_action_volume':'sum','outlook_action_volume':'sum',\n",
    "        'global_isps_action_volume':'sum','overall_action_volume':'sum',\n",
    "        'Contacts Lost':'sum',\n",
    "        'Date':'max',\n",
    "    }).reset_index(drop = False)\n",
    "\n",
    "    # re-calculate rate cols\n",
    "    groupMaster['Open rate'] = groupMaster['Opens']/groupMaster['Delivered']\n",
    "    groupMaster['Click rate'] = groupMaster['Clicks']/groupMaster['Opens']\n",
    "    groupMaster['Adjusted Click Rate'] = groupMaster['Adjusted Clicks']/groupMaster['Opens']\n",
    "    groupMaster['EPC'] = groupMaster['Revenue']/groupMaster['Clicks']\n",
    "    groupMaster['eCPM'] = groupMaster['Revenue']*1000/groupMaster['Delivered']\n",
    "    groupMaster['Contact loss rate'] = groupMaster['Contacts Lost']/groupMaster['Delivered']\n",
    "    for j in ['aol', 'google', 'yahoo', 'outlook', 'global_isps', 'overall']:\n",
    "        groupMaster['%s_inbox_percent' % j] = groupMaster['%s_action_volume' % j] / groupMaster['actual_%s_volume' % j]\n",
    "\n",
    "    groupMaster=groupMaster.rename(columns={'Date':'Last seen','Message':'Number of drops','openers':'Segment List'})\n",
    "\n",
    "\n",
    "    # add offer info\n",
    "    groupMaster = pd.merge(groupMaster, offers[['Hitpath Offer ID','Offer Name','Vertical']],\n",
    "                          how = 'left',\n",
    "                          left_on = 'Campaign ID',\n",
    "                          right_on = 'Hitpath Offer ID').drop('Hitpath Offer ID',axis = 1)\n",
    "    \n",
    "    # select cols to keep\n",
    "    groupMaster=groupMaster[['Last seen','Campaign ID','Offer Name','Vertical','subject line_rev',\n",
    "                             'Segment List','Number of drops','Sent','Delivered', 'Opens', 'Open rate','Clicks','Click rate',\n",
    "                             'Contacts Lost',\n",
    "                             'Contact loss rate',\n",
    "                             'Adjusted Clicks',\n",
    "                             'Adjusted Click Rate',\n",
    "                             'Revenue','EPC', 'eCPM',\n",
    "                             'google_inbox_percent', 'yahoo_inbox_percent', 'outlook_inbox_percent',\n",
    "                             'global_isps_inbox_percent', 'overall_inbox_percent',\n",
    "                            ]]\n",
    "    return groupMaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def productionAnalysis(weekly_drops,offers,folder_name,subfolder,prefix,fullTime,reportName):\n",
    "    \"\"\"\n",
    "    Create production analysis report\n",
    "    Output:\n",
    "       Offer_Vertical Production Analysis-COMBINED-02.02.20 - 03.03.20.xlsx \n",
    "    \"\"\"\n",
    "    o30WeeklyDrops = weekly_drops[weekly_drops.openers == 'O30']\n",
    "    o30weeklyDrops_offer, s_ver_1, third_final = irvine_report_main(o30WeeklyDrops, offers)\n",
    "\n",
    "    o30writer = pd.ExcelWriter(f'/Users/yanangao/Desktop/{folder_name}/{subfolder}/Offer_Vertical Production Analysis-{reportName}-{fullTime}.xlsx', engine = 'xlsxwriter')\n",
    "    for vert in list(o30weeklyDrops_offer.Vertical.dropna().unique()):\n",
    "        creditTable = o30weeklyDrops_offer[o30weeklyDrops_offer.Vertical == vert].sort_values(['Revenue'],ascending = False)\n",
    "        creditRatio = creditTable[[\n",
    "            'Hitpath Offer ID','Offer Name','Vertical','Operational Status','Advertiser Name','Payout',\n",
    "            'Payout Type','Budget','Cap','Day Restrictions','Campaign ID',\n",
    "        ]]\n",
    "\n",
    "    #     print(vert)\n",
    "        for colHeader in ['Delivered','Opens','Open rate','Clicks','Click rate','Adjusted Clicks','Adjusted Click Rate','Revenue','EPC','eCPM']:\n",
    "            creditRatio[colHeader] = (creditTable[colHeader]-creditTable.iloc[0][colHeader])/creditTable.iloc[0][colHeader]\n",
    "\n",
    "    #     print('done')   \n",
    "\n",
    "        creditTable.to_excel(o30writer,sheet_name=f'{vert}',startrow=0 , startcol=0)   \n",
    "        creditRatio.to_excel(o30writer,sheet_name=f'{vert}',startrow=len(creditTable)+3, startcol=0) \n",
    "        \n",
    "        \n",
    "    o30writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedulebyContentID(weekly_dropsIrvine):\n",
    "    \"\"\"\n",
    "    create content split report\n",
    "    Output:\n",
    "        schedulMaster: check \"Content Split Report_02.02.20 - 03.03.20.xlsx\"\n",
    "    \"\"\"\n",
    "    weekly_dropsIrvine['Margin'] = weekly_dropsIrvine.Margin.replace('no data',np.nan).astype(float)\n",
    "    scheduleMaster = weekly_dropsIrvine[weekly_dropsIrvine['Content ID']!='HTML']\n",
    "    scheduleMaster = scheduleMaster[weekly_dropsIrvine['Content ID']!='CC']\n",
    "    schedulMaster = scheduleMaster.groupby(['Campaign ID','Date','data_provider','openers','Content ID']).agg({\n",
    "        'Message':'; '.join,\n",
    "        'Sent':'sum',\n",
    "        'Delivered':'sum',\n",
    "        'Opens':'sum',\n",
    "        'Clicks':'sum',\n",
    "        'Contacts Lost':'sum',\n",
    "        'Adjusted Clicks':'sum',\n",
    "        'Offer Name':'max',\n",
    "        'Link Used in Mailing':'max',\n",
    "        'Affiliate ID':'max',\n",
    "        'Sub ID':'; '.join, #errors when there are NA values\n",
    "        'Revenue':'sum',\n",
    "        'Cost CPM':'max',\n",
    "        'Conversions': 'sum',\n",
    "        'Cost per send':'sum',\n",
    "        'Net Revenue':'sum',\n",
    "        'Margin':'mean',\n",
    "        'actual_aol_volume':'sum',\n",
    "        'actual_google_volume':'sum',\n",
    "        'actual_yahoo_volume':'sum',\n",
    "        'actual_outlook_volume':'sum',\n",
    "        'actual_global_isps_volume':'sum',\n",
    "        'actual_overall_volume':'sum',\n",
    "        'aol_action_volume':'sum',\n",
    "        'google_action_volume':'sum',\n",
    "        'yahoo_action_volume':'sum',\n",
    "        'outlook_action_volume':'sum',\n",
    "        'global_isps_action_volume':'sum',\n",
    "        'overall_action_volume':'sum',\n",
    "#         'esp':'; '.join,\n",
    "#         'sa':'; '.join,\n",
    "        'Affiliate ID':'first',\n",
    "\n",
    "    }).reset_index(drop=False).sort_values(['Date'])\n",
    "\n",
    "    schedulMaster['Open Rate'] = schedulMaster['Opens']/schedulMaster['Delivered']\n",
    "    schedulMaster['Click Rate'] = schedulMaster['Clicks']/schedulMaster['Opens']\n",
    "    schedulMaster['Adjusted Click Rate'] = schedulMaster['Adjusted Clicks']/schedulMaster['Opens']\n",
    "    schedulMaster['RPC'] = schedulMaster['Revenue']/schedulMaster['Clicks']\n",
    "    schedulMaster['Revenue CPM (eCPM)'] = schedulMaster['Revenue']*1000/schedulMaster['Delivered']\n",
    "    schedulMaster['CLR'] = schedulMaster['Contacts Lost']/schedulMaster['Opens']\n",
    "    for j in ['aol', 'google', 'yahoo', 'outlook', 'global_isps', 'overall']:\n",
    "        schedulMaster['%s_inbox_percent' % j] = schedulMaster['%s_action_volume' % j] / schedulMaster['actual_%s_volume' % j]\n",
    "\n",
    "    attr = ['Campaign ID','Date',  'data_provider', 'openers', 'Message','Sent', 'Delivered', 'Opens', 'Open Rate', 'Clicks',\n",
    "            'Click Rate', 'Contacts Lost','CLR','Adjusted Clicks','Adjusted Click Rate','Offer Name', 'Campaign ID', 'Link Used in Mailing', 'Affiliate ID', 'Sub ID',\n",
    "            'Revenue', 'RPC', 'Revenue CPM (eCPM)', 'Conversions', 'Cost CPM', 'Cost per send', 'Net Revenue', 'Margin',\n",
    "             'overall_inbox_percent', 'aol_inbox_percent','google_inbox_percent', 'yahoo_inbox_percent', 'outlook_inbox_percent',\n",
    "            'global_isps_inbox_percent',\n",
    "#             'esp',\n",
    "#             'sa',\n",
    "            'Affiliate ID',\n",
    "            'Content ID']\n",
    "\n",
    "    schedulMaster = schedulMaster[attr]\n",
    "    schedulMaster = schedulMaster.rename(columns={'Campaign ID':'Offer ID'})\n",
    "    schedulMaster.insert(2, \"Helper\", np.nan)\n",
    "#     schedulMaster.insert(16,\"Sub ID\",np.nan)\n",
    "    schedulMaster.insert(21,\"Unnamed: 15\",np.nan)\n",
    "    schedulMaster.insert(30,\"Vertical\", np.nan)\n",
    "    schedulMaster.insert(31,\"Network\", np.nan)\n",
    "    \n",
    "    return schedulMaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def schedulingmaster(weekly_dropsIrvine):\n",
    "#     \"\"\"\n",
    "#     Combine stats for one split message drop in Bronto and Iterable. \n",
    "#     \"\"\"\n",
    "#     weekly_dropsIrvine['Margin'] = weekly_dropsIrvine.Margin.replace('no data',np.nan).astype(float)\n",
    "#     schedulMaster = weekly_dropsIrvine.groupby(['Campaign ID','Date','data_provider','openers']).agg({\n",
    "#         'Message':'; '.join,\n",
    "#         'Sent':'sum',\n",
    "#         'Delivered':'sum',\n",
    "#         'Opens':'sum',\n",
    "#         'Clicks':'sum',\n",
    "#         'Contacts Lost':'sum',\n",
    "#         'Adjusted Clicks':'sum',\n",
    "#         'Offer Name':'max',\n",
    "#     #     'Campaign ID':'max',\n",
    "#         'Link Used in Mailing':'max',\n",
    "#         'Affiliate ID':'max',\n",
    "#         'Sub ID':'; '.join, #errors!!!!?????\n",
    "#         'Revenue':'sum',\n",
    "#         'Cost CPM':'max',\n",
    "#         'Cost per send':'sum',\n",
    "#         'Net Revenue':'sum',\n",
    "#         'Margin':'mean',\n",
    "#         'actual_aol_volume':'sum',\n",
    "#         'actual_google_volume':'sum',\n",
    "#         'actual_yahoo_volume':'sum',\n",
    "#         'actual_outlook_volume':'sum',\n",
    "#         'actual_global_isps_volume':'sum',\n",
    "#         'actual_overall_volume':'sum',\n",
    "#         'aol_action_volume':'sum',\n",
    "#         'google_action_volume':'sum',\n",
    "#         'yahoo_action_volume':'sum',\n",
    "#         'outlook_action_volume':'sum',\n",
    "#         'global_isps_action_volume':'sum',\n",
    "#         'overall_action_volume':'sum',\n",
    "#         'drop':'; '.join,\n",
    "#         'ESP':';' .join,\n",
    "\n",
    "#     }).reset_index(drop=False)\n",
    "\n",
    "#     schedulMaster['Open Rate'] = schedulMaster['Opens']/schedulMaster['Delivered']\n",
    "#     schedulMaster['Click Rate'] = schedulMaster['Clicks']/schedulMaster['Opens']\n",
    "#     schedulMaster['Adjusted Click Rate'] = schedulMaster['Adjusted Clicks']/schedulMaster['Opens']\n",
    "#     schedulMaster['RPC'] = schedulMaster['Revenue']/schedulMaster['Clicks']\n",
    "#     schedulMaster['Revenue CPM (eCPM)'] = schedulMaster['Revenue']*1000/schedulMaster['Delivered']\n",
    "#     for j in ['aol', 'google', 'yahoo', 'outlook', 'global_isps', 'overall']:\n",
    "#         schedulMaster['%s_inbox_percent' % j] = schedulMaster['%s_action_volume' % j] / schedulMaster['actual_%s_volume' % j]\n",
    "\n",
    "#     attr = ['Campaign ID','Date',  'data_provider', 'openers', 'Message','Sent', 'Delivered', 'Opens', 'Open Rate', 'Clicks',\n",
    "#             'Click Rate', 'Contacts Lost','Adjusted Clicks','Adjusted Click Rate','Offer Name', 'Campaign ID', 'Link Used in Mailing', 'Affiliate ID', 'Sub ID',\n",
    "#             'Revenue', 'RPC', 'Revenue CPM (eCPM)', 'Cost CPM', 'Cost per send', 'Net Revenue', 'Margin',\n",
    "#              'overall_inbox_percent', 'aol_inbox_percent','google_inbox_percent', 'yahoo_inbox_percent', 'outlook_inbox_percent',\n",
    "#             'global_isps_inbox_percent','drop','ESP']\n",
    "\n",
    "#     schedulMaster = schedulMaster[attr]\n",
    "#     schedulMaster = schedulMaster.rename(columns={'Campaign ID':'Offer ID'})\n",
    "#     schedulMaster.insert(2, \"Helper\", np.nan)\n",
    "# #     schedulMaster.insert(16,\"Sub ID\",np.nan)\n",
    "#     schedulMaster.insert(20,\"Unnamed: 15\",np.nan)\n",
    "#     schedulMaster.insert(28,\"Vertical\", np.nan)\n",
    "#     schedulMaster.insert(29,\"Network\", np.nan)\n",
    "    \n",
    "#     return schedulMaster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def revenueSummation(full_drops, offers):\n",
    "    \n",
    "#     # revenue summary campaign id level\n",
    "#     full_drops['week_number'] = full_drops.Date.dt.week\n",
    "#     revSumDF = full_drops.groupby(['Campaign ID','week_number']).aggregate({'Revenue':'sum'}).reset_index(drop = False)\n",
    "#     revSumIDview = revSumDF.pivot_table(\n",
    "#         'Revenue',index = 'Campaign ID',columns='week_number',\n",
    "#         aggfunc=np.sum, margins=True\n",
    "#     )\n",
    "    \n",
    "#     # revenue summary advertiser level\n",
    "#     offersAdv = offers[['Hitpath Offer ID','Advertiser Name']]\n",
    "#     fullDropsAddMonthAdv = pd.merge(\n",
    "#         full_drops, offersAdv,\n",
    "#         how = 'left',\n",
    "#         left_on = 'Campaign ID',\n",
    "#         right_on = 'Hitpath Offer ID'\n",
    "#     )\n",
    "#     fullDropsAddMonthAdv['month'] = fullDropsAddMonthAdv.Date.dt.month\n",
    "#     revSumAdvView = fullDropsAddMonthAdv.groupby(['month','week_number','Advertiser Name']).aggregate({'Revenue':'sum'})\n",
    "#     revSumAdvView = revSumAdvView.pivot_table(\n",
    "#         'Revenue',\n",
    "#         index = ['month','week_number'],\n",
    "#         columns = 'Advertiser Name'\n",
    "#     )\n",
    "#     return revSumIDview,revSumAdvView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offerSummaryByDatasets(full_drops, offers,folder_nameIrvine):\n",
    "    '''\n",
    "    create Offer Summary Stats By Dataset report\n",
    "    Output: check Offer Summary Stats By Dataset.xlsx\n",
    "    '''\n",
    "    o30Full = full_drops[full_drops.openers == 'O30']\n",
    "    masterAddVertical = pd.merge(offers[['Hitpath Offer ID','Vertical','Payout']],o30Full,how='left', \n",
    "                                 left_on = 'Hitpath Offer ID', right_on = 'Campaign ID')\n",
    "    pivotTable = ExcelWriter(f'/Users/yanangao/Desktop/{folder_nameIrvine}/Irvine Reports/Offer Summary Stats By Dataset.xlsx')\n",
    "\n",
    "    for data in list(masterAddVertical.data_provider.dropna().unique()):\n",
    "        subDrops = masterAddVertical[masterAddVertical.data_provider == data]\n",
    "        summaryStats = subDrops.assign(\n",
    "            result1 = np.where(subDrops['Contacts Lost']>0, subDrops.Delivered, 0)\n",
    "        ).groupby(['Campaign ID','Vertical','Payout'], as_index = False).agg({\n",
    "            'Sent':'sum',\n",
    "            'Delivered':'sum',\n",
    "            'Revenue':'sum',\n",
    "            'Opens':'sum',\n",
    "            'Open Rate':['mean','std'],\n",
    "            'Clicks':'sum',\n",
    "            'Click Rate':['mean','std'],\n",
    "            'Adjusted Clicks':'sum',\n",
    "            'Adjusted Click Rate':['mean','std'],\n",
    "            'result1':'sum',\n",
    "            'Contacts Lost':'sum',\n",
    "            'Message':'count'\n",
    "        })\n",
    "\n",
    "\n",
    "        # rename column names\n",
    "        d = {\n",
    "            'Sentsum':'Total Sent',\n",
    "            'Deliveredsum':'Total Delivered',\n",
    "            'Revenuesum':'Total Revenue',\n",
    "            'Openssum':'Sum Opens',\n",
    "            'Open Ratemean':'Average Open Rate by Offer',\n",
    "            'Open Ratestd':'StdDev Open Rate',\n",
    "            'Clickssum':'Total Production Clicks',\n",
    "            'Click Ratemean':'Average Click Rate by offer',\n",
    "            'Click Ratestd':'StdDev Click Rate by Offer',\n",
    "            'Adjusted Clickssum':'Total Adjusted Clicks',\n",
    "            'Adjusted Click Ratemean':'Average Adjusted Click Rate by offer',\n",
    "            'Adjusted Click Ratestd':'StdDev Adjusted Click Rate by Offer',\n",
    "            'result1sum':'Sent (Contacts Lost Purposes)',\n",
    "            'Contacts Lostsum':'Sum Contacts Lost',\n",
    "            'Messagecount':'Total Drops',   \n",
    "        }\n",
    "        summaryStats.columns = summaryStats.columns.map(''.join)\n",
    "        summaryStats = summaryStats.reset_index().rename(columns=d)\n",
    "\n",
    "\n",
    "        # add calculated columns\n",
    "        summaryStats['Calculated Avg Historical eCPM AP'] = summaryStats['Total Revenue']*1000/summaryStats['Total Delivered']\n",
    "        summaryStats['Contact Lost Rate'] = summaryStats['Sum Contacts Lost']/summaryStats['Sent (Contacts Lost Purposes)']\n",
    "        summaryStats['EPC'] = summaryStats['Total Revenue']/summaryStats['Total Production Clicks']\n",
    "        summaryStats['Click Through Rate'] = summaryStats['Average Open Rate by Offer']*summaryStats['Average Click Rate by offer']\n",
    "\n",
    "        # rearrange columns\n",
    "        header_list = [\n",
    "            'Campaign ID', 'Vertical', 'Total Sent', 'Total Delivered','Total Revenue', \n",
    "            'Sum Opens', 'Average Open Rate by Offer', 'StdDev Open Rate',\n",
    "            'Average Click Rate by offer', 'Total Production Clicks', 'StdDev Click Rate by Offer', \n",
    "            'Total Adjusted Clicks','Average Adjusted Click Rate by offer','StdDev Adjusted Click Rate by Offer',\n",
    "            'Median Offer eCPM StdDev', 'Calculated Avg Historical eCPM AP',\n",
    "            'Sent (Contacts Lost Purposes)', 'Sum Contacts Lost',\n",
    "            'Contact Lost Rate', 'EPC', 'Click Through Rate', 'Conversions','Total Drops','Payout',\n",
    "        ]\n",
    "        summaryStats = summaryStats.reindex(columns = header_list)\n",
    "\n",
    "        # data type and formatting\n",
    "        summaryStats['Campaign ID'] = summaryStats['Campaign ID'].astype(int)\n",
    "        \n",
    "        #save in excel file\n",
    "        summaryStats.to_excel(pivotTable,f'{data}', index = False)\n",
    "        \n",
    "        format(summaryStats,pivotTable,f'{data}')\n",
    "    pivotTable.save()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def GroupBySubaccount(weekly_drops):\n",
    "#     '''\n",
    "    \n",
    "#     '''\n",
    "#     weeklyDropsSubUse = weekly_drops.copy()\n",
    "#     weeklyDropsSubUse = weeklyDropsSubUse[weeklyDropsSubUse.Date >= '2019-08-19']\n",
    "#     weeklyDropsSubUse['ESP'] = weeklyDropsSubUse['Sub ID'].str.split(\"_\",expand = True)[0]\n",
    "#     weeklyDropsSubUse['Sub_account'] = weeklyDropsSubUse['Sub ID'].str.split(\"_\",expand = True)[1]\n",
    "#     weeklyDropsSubUse['ESP_SA'] = weeklyDropsSubUse[['ESP','Sub_account']].apply(lambda x:'_'.join(x),axis = 1)\n",
    "\n",
    "#     # weeklyDropsSubUse\n",
    "\n",
    "#     subaccountWise = weeklyDropsSubUse.groupby(['ESP','Sub_account'], as_index = False).agg({\n",
    "#         'Message':'count',\n",
    "#         'Sent':'sum',\n",
    "#         'Delivered':'sum',\n",
    "#         'Opens':'sum',\n",
    "#         'Clicks':'sum',\n",
    "#         'Adjusted Clicks':'sum',\n",
    "#         'Revenue':'sum',\n",
    "#     })\n",
    "#     subaccountWise['Open_rate'] = subaccountWise['Opens']/subaccountWise['Delivered']\n",
    "#     subaccountWise['Click_rate'] = subaccountWise['Clicks']/subaccountWise['Opens']\n",
    "#     subaccountWise['Adjusted Clicks_rate'] = subaccountWise['Adjusted Clicks']/subaccountWise['Opens']\n",
    "#     subaccountWise['ECPM'] = subaccountWise['Revenue']*1000/subaccountWise['Delivered']\n",
    "#     subaccountWise['EPC'] = subaccountWise['Revenue']/subaccountWise['Clicks']\n",
    "\n",
    "#     subaccountWise = subaccountWise.rename(columns = {'Message':'# Drops'})\n",
    "#     colHeaders = ['ESP','Sub_account','# Drops','Sent','Delivered','Opens','Open_rate','Clicks','Click_rate','Adjusted Clicks','Adjusted Clicks_rate','Revenue','ECPM','EPC','Notes']\n",
    "#     subaccountWise = subaccountWise.reindex(columns = colHeaders)\n",
    "#     return weeklyDropsSubUse, subaccountWise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
